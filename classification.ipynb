{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Distance Calculation Function\n",
    "This function computes the distance matrix between train and test sets using either cosine similarity or Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_distance_matrix(X_train, X_test, metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Calculate distance matrix between training and testing sets using the specified metric.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.array): Training data (TF-IDF features).\n",
    "        X_test (np.array): Test data (TF-IDF features).\n",
    "        metric (str): Distance metric (\"cosine\" or \"euclidean\").\n",
    "\n",
    "    Returns:\n",
    "        np.array: Distance matrix of shape (len(X_test), len(X_train)).\n",
    "    \"\"\"\n",
    "    if metric == \"cosine\":\n",
    "        return cosine_distances(X_test, X_train)\n",
    "    elif metric == \"euclidean\":\n",
    "        return euclidean_distances(X_test, X_train)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported metric. Use 'cosine' or 'euclidean'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. k-NN Prediction\n",
    "This function predicts labels for the test set by finding the k-nearest neighbors and performing majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(X_train, y_train, X_test, k, distance_matrix):\n",
    "    \"\"\"\n",
    "    Predict labels for test data using k-NN.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.array): Training data (TF-IDF features).\n",
    "        y_train (np.array): Training labels.\n",
    "        X_test (np.array): Test data (TF-IDF features).\n",
    "        k (int): Number of neighbors.\n",
    "        distance_matrix (np.array): Precomputed distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Predicted labels for X_test.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in range(distance_matrix.shape[0]):\n",
    "        # Get the k nearest neighbors\n",
    "        neighbors_idx = np.argsort(distance_matrix[i])[:k]\n",
    "        neighbors_labels = y_train[neighbors_idx]\n",
    "        # Majority voting\n",
    "        predicted_label = np.bincount(neighbors_labels).argmax()\n",
    "        predictions.append(predicted_label)\n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Metrics\n",
    "This function manually calculates precision, recall, F1-Score, and true/false positives/negatives for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_metrics(y_true, y_pred, classes):\n",
    "    \"\"\"\n",
    "    Manually calculate Precision, Recall, F1-Score, TP, FP, FN for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "        classes (list): List of unique class labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with metrics for each class and macro/micro averages.\n",
    "    \"\"\"\n",
    "    metrics = defaultdict(dict)\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "\n",
    "    for cls in classes:\n",
    "        tp = np.sum((y_true == cls) & (y_pred == cls))\n",
    "        fp = np.sum((y_true != cls) & (y_pred == cls))\n",
    "        fn = np.sum((y_true == cls) & (y_pred != cls))\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "        metrics[cls][\"TP\"] = tp\n",
    "        metrics[cls][\"FP\"] = fp\n",
    "        metrics[cls][\"FN\"] = fn\n",
    "        metrics[cls][\"Precision\"] = precision\n",
    "        metrics[cls][\"Recall\"] = recall\n",
    "        metrics[cls][\"F1\"] = f1\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "    # Macro averages\n",
    "    macro_precision = np.mean([metrics[cls][\"Precision\"] for cls in classes])\n",
    "    macro_recall = np.mean([metrics[cls][\"Recall\"] for cls in classes])\n",
    "    macro_f1 = np.mean([metrics[cls][\"F1\"] for cls in classes])\n",
    "\n",
    "    # Micro averages\n",
    "    micro_precision = total_tp / (total_tp + total_fp)\n",
    "    micro_recall = total_tp / (total_tp + total_fn)\n",
    "    micro_f1 = (\n",
    "        2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "        if (micro_precision + micro_recall) > 0\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    metrics[\"Macro Average\"] = {\n",
    "        \"Precision\": macro_precision,\n",
    "        \"Recall\": macro_recall,\n",
    "        \"F1\": macro_f1,\n",
    "    }\n",
    "    metrics[\"Micro Average\"] = {\n",
    "        \"Precision\": micro_precision,\n",
    "        \"Recall\": micro_recall,\n",
    "        \"F1\": micro_f1,\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. k-NN with Cross-Validation\n",
    "Perform stratified 10-fold cross-validation and evaluate k-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_with_cross_validation(X, y, k_values, metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Perform stratified 10-fold cross-validation with k-NN.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): TF-IDF feature matrix.\n",
    "        y (np.array): Class labels.\n",
    "        k_values (list): List of k values to evaluate.\n",
    "        metric (str): Distance metric (\"cosine\" or \"euclidean\").\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing metrics for each k value.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    for k in k_values:\n",
    "        fold_metrics = []\n",
    "        for train_idx, test_idx in skf.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Compute distance matrix\n",
    "            distance_matrix = calculate_distance_matrix(X_train, X_test, metric)\n",
    "\n",
    "            # Predict labels\n",
    "            y_pred = knn_predict(X_train, y_train, X_test, k, distance_matrix)\n",
    "\n",
    "            # Compute metrics\n",
    "            metrics = manual_metrics(y_test, y_pred, classes)\n",
    "            fold_metrics.append(metrics)\n",
    "\n",
    "        # Average metrics across folds\n",
    "        avg_metrics = {\n",
    "            cls: {key: np.mean([fold[cls][key] for fold in fold_metrics]) for key in fold_metrics[0][cls]}\n",
    "            for cls in classes\n",
    "        }\n",
    "        avg_metrics[\"Macro Average\"] = fold_metrics[0][\"Macro Average\"]\n",
    "        avg_metrics[\"Micro Average\"] = fold_metrics[0][\"Micro Average\"]\n",
    "\n",
    "        results[k] = avg_metrics\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run k-NN Classification\n",
    "Set up data paths, load the dataset, and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TF-IDF matrix with labels in the last column\n",
    "tfidf_path = \"reports/tfidf_values.csv\"  # Update to the actual path\n",
    "tfidf_data = pd.read_csv(tfidf_path, index_col=0)\n",
    "\n",
    "# Separate features (TF-IDF matrix) and labels\n",
    "X = tfidf_data.iloc[:, :-1].to_numpy()  # All columns except the last are TF-IDF features\n",
    "y = tfidf_data.iloc[:, -1].to_numpy()   # The last column contains the labels\n",
    "\n",
    "# Validate dimensions\n",
    "assert X.shape[0] == len(y), \"The number of samples in TF-IDF and labels must match!\"\n",
    "\n",
    "# Define k values and metric\n",
    "k_values = [1, 3, 5, 7, 9]\n",
    "metric = \"cosine\"\n",
    "\n",
    "# Run k-NN cross-validation\n",
    "results = knn_with_cross_validation(X, y, k_values, metric)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df.to_csv(\"reports/performance_metrics.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results of k-NN obtained by: k = 1, similarity metric = cosine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>True Positives</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Class 1</td>\n",
       "      <td>0.411675</td>\n",
       "      <td>0.519754</td>\n",
       "      <td>0.459133</td>\n",
       "      <td>39.3</td>\n",
       "      <td>56.3</td>\n",
       "      <td>36.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Class 2</td>\n",
       "      <td>0.607908</td>\n",
       "      <td>0.508164</td>\n",
       "      <td>0.553084</td>\n",
       "      <td>65.4</td>\n",
       "      <td>42.2</td>\n",
       "      <td>63.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Class 3</td>\n",
       "      <td>0.399794</td>\n",
       "      <td>0.402686</td>\n",
       "      <td>0.400688</td>\n",
       "      <td>38.5</td>\n",
       "      <td>58.2</td>\n",
       "      <td>57.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Macro Average</td>\n",
       "      <td>0.498381</td>\n",
       "      <td>0.501873</td>\n",
       "      <td>0.495386</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Micro Average</td>\n",
       "      <td>0.503333</td>\n",
       "      <td>0.503333</td>\n",
       "      <td>0.503333</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Metric  Precision    Recall   F-Score True Positives  \\\n",
       "0        Class 1   0.411675  0.519754  0.459133           39.3   \n",
       "1        Class 2   0.607908  0.508164  0.553084           65.4   \n",
       "2        Class 3   0.399794  0.402686  0.400688           38.5   \n",
       "3  Macro Average   0.498381  0.501873  0.495386              -   \n",
       "4  Micro Average   0.503333  0.503333  0.503333              -   \n",
       "\n",
       "  False Positives False Negatives  \n",
       "0            56.3            36.3  \n",
       "1            42.2            63.3  \n",
       "2            58.2            57.1  \n",
       "3               -               -  \n",
       "4               -               -  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The detailed k-NN report has been saved to: reports/detailed_knn_report.csv\n"
     ]
    }
   ],
   "source": [
    "def generate_detailed_report(results, k_values, similarity_metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Generate a detailed report for the best k value.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Dictionary of performance metrics for each k.\n",
    "        k_values (list): List of tested k values.\n",
    "        similarity_metric (str): The similarity metric used.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Detailed report in the required format.\n",
    "    \"\"\"\n",
    "    # Find the best k based on Macro Average F1\n",
    "    best_k = max(k_values, key=lambda k: results[k][\"Macro Average\"][\"F1\"])\n",
    "    best_metrics = results[best_k]\n",
    "\n",
    "    # Create the report table\n",
    "    metrics = [\"Precision\", \"Recall\", \"F-Score\", \n",
    "               \"Total no. of True Positive records\", \n",
    "               \"Total no. of False Positive records\", \n",
    "               \"Total no. of False Negative records\"]\n",
    "    rows = []\n",
    "\n",
    "    # Add data for each class\n",
    "    for cls in range(1, 4):  # Assuming Class 1, Class 2, Class 3\n",
    "        cls_metrics = best_metrics[cls]\n",
    "        rows.append([\n",
    "            f\"Class {cls}\",\n",
    "            cls_metrics[\"Precision\"],\n",
    "            cls_metrics[\"Recall\"],\n",
    "            cls_metrics[\"F1\"],\n",
    "            cls_metrics[\"TP\"],\n",
    "            cls_metrics[\"FP\"],\n",
    "            cls_metrics[\"FN\"]\n",
    "        ])\n",
    "\n",
    "    # Add Macro and Micro averages\n",
    "    for avg_type in [\"Macro Average\", \"Micro Average\"]:\n",
    "        avg_metrics = best_metrics[avg_type]\n",
    "        rows.append([\n",
    "            avg_type,\n",
    "            avg_metrics[\"Precision\"],\n",
    "            avg_metrics[\"Recall\"],\n",
    "            avg_metrics[\"F1\"],\n",
    "            \"-\", \"-\", \"-\"  # No TP/FP/FN for averages\n",
    "        ])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    report_df = pd.DataFrame(rows, columns=[\n",
    "        \"Metric\", \"Precision\", \"Recall\", \"F-Score\", \n",
    "        \"True Positives\", \"False Positives\", \"False Negatives\"\n",
    "    ])\n",
    "\n",
    "    # Add metadata\n",
    "    report_metadata = f\"Best results of k-NN obtained by: k = {best_k}, similarity metric = {similarity_metric}\"\n",
    "\n",
    "    return report_df, report_metadata\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "k_values = [1, 3, 5, 7, 9]  # Example k values\n",
    "report_df, report_metadata = generate_detailed_report(results, k_values, similarity_metric=\"cosine\")\n",
    "\n",
    "# Display the metadata and DataFrame\n",
    "print(report_metadata)\n",
    "display(report_df)  # Print the DataFrame in tabular form\n",
    "\n",
    "# Save the report to a CSV file\n",
    "report_path = \"reports/detailed_knn_report.csv\"\n",
    "report_df.to_csv(report_path, index=False)\n",
    "print(f\"The detailed k-NN report has been saved to: {report_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
