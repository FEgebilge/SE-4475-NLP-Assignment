{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Turkish Tweets with Zemberek\n",
    "\n",
    "This notebook implements the preprocessing step of the project. It includes tokenization, lowercasing, stemming using Zemberek, and optional stop-word removal. The processed tweets will be saved for later steps, such as TF-IDF transformation and classification.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from py4j.java_gateway import JavaGateway, GatewayParameters, launch_gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Zemberek for Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_zemberek(jar_path=\"zemberek-full.jar\"):\n",
    "    \"\"\"Starts Zemberek NLP through Py4J gateway.\"\"\"\n",
    "    port = launch_gateway(classpath=jar_path)\n",
    "    gateway = JavaGateway(gateway_parameters=GatewayParameters(port=port))\n",
    "    tokenizer = gateway.jvm.zemberek.tokenization.TurkishTokenizer.DEFAULT\n",
    "    extractor = gateway.jvm.zemberek.tokenization.TurkishSentenceExtractor.DEFAULT\n",
    "    return tokenizer, extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Tokenization, Lowercasing, and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, tokenizer, extractor, stop_words=[]):\n",
    "    \"\"\"\n",
    "    Preprocesses Turkish text using Zemberek. Tokenizes, extracts lemmas, \n",
    "    and removes stop words.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to preprocess.\n",
    "        tokenizer: Instance of TurkishTokenizer.\n",
    "        extractor: Instance of TurkishSentenceExtractor.\n",
    "        stop_words (list): List of stop words to exclude.\n",
    "\n",
    "    Returns:\n",
    "        list: List of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    # Extract sentences from text\n",
    "    sentences = extractor.fromDocument(text)\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize each sentence\n",
    "        token_strings = tokenizer.tokenizeToStrings(sentence)\n",
    "        \n",
    "        for token in token_strings:\n",
    "            # Check if token is not a stop word\n",
    "            if token not in stop_words:\n",
    "                tokens.append(token)\n",
    "\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process All Tweets in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(dataset_folder, tokenizer, extractor, stop_words=[]):\n",
    "    \"\"\"Processes all tweets in the dataset.\"\"\"\n",
    "    processed_data = {}\n",
    "    for label in os.listdir(dataset_folder):  # Iterate over 'Positive', 'Negative', 'Neutral'\n",
    "        label_folder = os.path.join(dataset_folder, label)\n",
    "        tweets = []\n",
    "        for filename in os.listdir(label_folder):  # Iterate over each tweet file\n",
    "            with open(os.path.join(label_folder, filename), \"r\", encoding=\"ISO-8859-9\") as file:\n",
    "                text = file.read()\n",
    "                processed = preprocess_text(text, tokenizer, extractor, stop_words)\n",
    "                tweets.append(processed)\n",
    "        processed_data[label] = tweets\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Zemberek\n",
    "tokenizer, extractor = initialize_zemberek()\n",
    "\n",
    "# Define stop-words (optional)\n",
    "turkish_stop_words = [\"ve\", \"bir\", \"ama\", \"Ã§ok\", \"gibi\"]  \n",
    "\n",
    "# Path to the dataset\n",
    "dataset_path = \"/Users/egebilge/Documents/Lectures/SE-4475 NLP/SE-4475-NLP-Assignment/data/raw_texts\"\n",
    "\n",
    "# Process the tweets\n",
    "processed_tweets = process_tweets(dataset_path, tokenizer, extractor, turkish_stop_words)\n",
    "\n",
    "# Print a sample\n",
    "for label, tweets in processed_tweets.items():\n",
    "    print(f\"\\nLabel: {label}\")\n",
    "    print(\"Sample Processed Tweet:\", tweets[0])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
