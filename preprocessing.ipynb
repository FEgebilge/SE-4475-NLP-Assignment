{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Turkish Tweets with Zemberek\n",
    "\n",
    "This notebook implements the preprocessing step of the project. It includes tokenization, lowercasing, stemming using Zemberek, and optional stop-word removal. The processed tweets will be saved for later steps, such as TF-IDF transformation and classification.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from py4j.java_gateway import JavaGateway, GatewayParameters, launch_gateway\n",
    "from helpers import load_turkish_stop_words_from_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Zemberek for Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_zemberek(jar_path=\"zemberek-full.jar\"):\n",
    "    \"\"\"Starts Zemberek NLP through Py4J gateway.\"\"\"\n",
    "    port = launch_gateway(classpath=jar_path)\n",
    "    gateway = JavaGateway(gateway_parameters=GatewayParameters(port=port))\n",
    "    tokenizer = gateway.jvm.zemberek.tokenization.TurkishTokenizer.DEFAULT\n",
    "    extractor = gateway.jvm.zemberek.tokenization.TurkishSentenceExtractor.DEFAULT\n",
    "    return tokenizer, extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Tokenization, Lowercasing, and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, tokenizer, extractor, stop_words=[]):\n",
    "    \"\"\"\n",
    "    Preprocesses Turkish text using Zemberek. Tokenizes, extracts lemmas, \n",
    "    and removes stop words.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to preprocess.\n",
    "        tokenizer: Instance of TurkishTokenizer.\n",
    "        extractor: Instance of TurkishSentenceExtractor.\n",
    "        stop_words (list): List of stop words to exclude.\n",
    "\n",
    "    Returns:\n",
    "        list: List of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    # Extract sentences from text\n",
    "    sentences = extractor.fromDocument(text)\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize each sentence\n",
    "        token_strings = tokenizer.tokenizeToStrings(sentence)\n",
    "        \n",
    "        for token in token_strings:\n",
    "            # Check if token is not a stop word\n",
    "            if token not in stop_words:\n",
    "                tokens.append(token)\n",
    "\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process All Tweets in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(dataset_folder, tokenizer, extractor, stop_words=[]):\n",
    "    \"\"\"\n",
    "    Processes all tweets in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_folder (str): Path to the dataset folder.\n",
    "        tokenizer: Instance of TurkishTokenizer.\n",
    "        extractor: Instance of TurkishSentenceExtractor.\n",
    "        stop_words (list): List of stop words to exclude.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with processed tweets by label.\n",
    "    \"\"\"\n",
    "    processed_data = {}\n",
    "\n",
    "    # Resolve absolute path for dataset folder\n",
    "    dataset_folder = os.path.abspath(dataset_folder)\n",
    "\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        raise FileNotFoundError(f\"Dataset folder not found: {dataset_folder}\")\n",
    "\n",
    "    # Iterate over label folders\n",
    "    for label in os.listdir(dataset_folder):\n",
    "        label_folder = os.path.join(dataset_folder, label)\n",
    "\n",
    "        # Skip files like `.DS_Store` and only process directories\n",
    "        if not os.path.isdir(label_folder):\n",
    "            print(f\"Skipping non-directory item: {label_folder}\")\n",
    "            continue\n",
    "\n",
    "        tweets = []\n",
    "\n",
    "        # Iterate over files in label folder\n",
    "        for filename in os.listdir(label_folder):\n",
    "            file_path = os.path.join(label_folder, filename)\n",
    "\n",
    "            # Skip non-text files if needed\n",
    "            if not file_path.endswith(\".txt\"):\n",
    "                print(f\"Skipping non-text file: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"ISO-8859-9\") as file:\n",
    "                    text = file.read()\n",
    "                    processed = preprocess_text(text, tokenizer, extractor, stop_words)\n",
    "                    tweets.append(processed)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Encoding error in file: {file_path}\")\n",
    "                continue\n",
    "\n",
    "        if tweets:  # Only add if there are tweets\n",
    "            processed_data[label] = tweets\n",
    "        else:\n",
    "            print(f\"No valid tweets found in folder: {label_folder}\")\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Zemberek\n",
    "tokenizer, extractor = initialize_zemberek()\n",
    "\n",
    "stop_words_csv_path = \"data/stop_words.csv\"\n",
    "#custom_stop_words = [\"turkcell\", \"fizy\"]\n",
    "stop_words = load_turkish_stop_words_from_csv(stop_words_csv_path)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the dataset\n",
    "dataset_path = \"data/raw_texts\"\n",
    "\n",
    "# Process the tweets\n",
    "processed_tweets = process_tweets(dataset_path, tokenizer, extractor, stop_words)\n",
    "\n",
    "# Print a sample\n",
    "for label, tweets in processed_tweets.items():\n",
    "    print(f\"\\nLabel: {label}\")\n",
    "    print(\"Sample Processed Tweet:\", tweets[0])\n",
    "\n",
    "# Save processed_tweets to a pickle file\n",
    "with open(\"data/processed_tweets.pkl\", \"wb\") as file:\n",
    "    pickle.dump(processed_tweets, file)\n",
    "\n",
    "print(\"Processed tweets saved to data/processed_tweets.pkl\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
