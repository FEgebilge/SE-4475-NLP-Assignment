{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Turkish Tweets with Zemberek\n",
    "\n",
    "This notebook implements the preprocessing step of the project. It includes tokenization, lowercasing, stemming using Zemberek, and optional stop-word removal. The processed tweets will be saved for later steps, such as TF-IDF transformation and classification.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from py4j.java_gateway import JavaGateway, GatewayParameters, launch_gateway\n",
    "from helpers import load_turkish_stop_words_from_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Zemberek for Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jpype import JClass, JString, getDefaultJVMPath, startJVM, shutdownJVM, java\n",
    "\n",
    "ZEMBEREK_PATH = \"zemberek-full.jar\"\n",
    "\n",
    "# Initialize JVM with Zemberek\n",
    "def initialize_zemberek():\n",
    "    startJVM(getDefaultJVMPath(), '-ea', f'-Djava.class.path={ZEMBEREK_PATH}')\n",
    "    TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n",
    "    morphology = TurkishMorphology.createWithDefaults()\n",
    "    return morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Tokenization, Lowercasing, and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, morphology, stop_words=[]):\n",
    "    \"\"\"\n",
    "    Preprocesses Turkish text using Zemberek: tokenizes, lemmatizes (stemming),\n",
    "    and removes stop words. Excludes tokens with lemma \"UNK\".\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to preprocess.\n",
    "        morphology: Initialized TurkishMorphology instance.\n",
    "        stop_words (list): List of stop words to exclude.\n",
    "\n",
    "    Returns:\n",
    "        list: List of preprocessed tokens (lemmas or original words).\n",
    "    \"\"\"\n",
    "    # Check for empty or whitespace-only text\n",
    "    if not text.strip():\n",
    "        return []  # Return an empty list if input is empty\n",
    "\n",
    "    # Analyze and disambiguate the text\n",
    "    analysis = morphology.analyzeAndDisambiguate(text).bestAnalysis()\n",
    "    tokens = []\n",
    "\n",
    "    for word_analysis in analysis:\n",
    "        try:\n",
    "            # Extract lemma\n",
    "            lemmas = word_analysis.getLemmas()\n",
    "            lemma = str(lemmas[0]) if lemmas else \"UNK\"  # Use \"UNK\" if no lemma is available\n",
    "        except Exception:\n",
    "            # In case of an unexpected error, set lemma as \"UNK\"\n",
    "            lemma = \"UNK\"\n",
    "\n",
    "        # Exclude \"UNK\" and stop words\n",
    "        if lemma != \"UNK\" and lemma not in stop_words:\n",
    "            tokens.append(lemma)\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process All Tweets in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(dataset_folder, morphology, stop_words=[]):\n",
    "    \"\"\"\n",
    "    Processes all tweets in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_folder (str): Path to the dataset folder.\n",
    "        morphology: Initialized TurkishMorphology instance for stemming.\n",
    "        stop_words (list): List of stop words to exclude.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with processed tweets by label.\n",
    "    \"\"\"\n",
    "    processed_data = {}\n",
    "\n",
    "    # Resolve absolute path for dataset folder\n",
    "    dataset_folder = os.path.abspath(dataset_folder)\n",
    "\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        raise FileNotFoundError(f\"Dataset folder not found: {dataset_folder}\")\n",
    "\n",
    "    # Iterate over label folders\n",
    "    for label in os.listdir(dataset_folder):\n",
    "        label_folder = os.path.join(dataset_folder, label)\n",
    "\n",
    "        # Skip files like `.DS_Store` and only process directories\n",
    "        if not os.path.isdir(label_folder):\n",
    "            print(f\"Skipping non-directory item: {label_folder}\")\n",
    "            continue\n",
    "\n",
    "        tweets = []\n",
    "\n",
    "        # Iterate over files in label folder\n",
    "        for filename in os.listdir(label_folder):\n",
    "            file_path = os.path.join(label_folder, filename)\n",
    "\n",
    "            # Skip non-text files if needed\n",
    "            if not file_path.endswith(\".txt\"):\n",
    "                print(f\"Skipping non-text file: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"ISO-8859-9\") as file:\n",
    "                    text = file.read().strip()  # Strip leading/trailing whitespace\n",
    "                    if not text:  # Skip empty files\n",
    "                        print(f\"Skipping empty file: {file_path}\")\n",
    "                        continue\n",
    "\n",
    "                    # Use the updated preprocess_text function with morphology\n",
    "                    processed = preprocess_text(text, morphology, stop_words)\n",
    "                    tweets.append(processed)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Encoding error in file: {file_path}\")\n",
    "                continue\n",
    "\n",
    "        if tweets:  # Only add if there are tweets\n",
    "            processed_data[label] = tweets\n",
    "        else:\n",
    "            print(f\"No valid tweets found in folder: {label_folder}\")\n",
    "\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neden', '', 've', 'bir', 'her', 'şu', 'ama', 'ne', 'o', 'bu', 'çok', 'nasıl', 'çünkü', '...', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I|14:06:23.848|Root lexicon created in 268 ms.                                                                     | DictionarySerializer#getDictionaryItems\n",
      "I|14:06:23.850|Dictionary generated in 338 ms                                                                      | RootLexicon#defaultBinaryLexicon\n",
      "I|14:06:24.028|Initialized in 554 ms.                                                                              | TurkishMorphology#createWithDefaults\n"
     ]
    }
   ],
   "source": [
    "# Initialize Zemberek\n",
    "morphology = initialize_zemberek()\n",
    "\n",
    "\n",
    "stop_words_csv_path = \"data/stop_words.csv\"\n",
    "#custom_stop_words = [\"turkcell\", \"fizy\"]\n",
    "stop_words = load_turkish_stop_words_from_csv(stop_words_csv_path)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-directory item: /Users/egebilge/Documents/Lectures/SE-4475 NLP/SE-4475-NLP-Assignment/data/raw_texts/.DS_Store\n",
      "Skipping empty file: /Users/egebilge/Documents/Lectures/SE-4475 NLP/SE-4475-NLP-Assignment/data/raw_texts/3/969.txt\n",
      "\n",
      "Label: 1\n",
      "Sample Processed Tweet: ['biraz', 'daha', 'geliş', 'fizy', 'yi', 'tamamen', 'söz']\n",
      "\n",
      "Label: 3\n",
      "Sample Processed Tweet: ['16', 'aralık', 'turkcell', '!']\n",
      "\n",
      "Label: 2\n",
      "Sample Processed Tweet: ['a', 'şimdi', 'derin', 'nefes', 'al', ',', 'beyin', 'oksijen', 'git', 'ayrıca', 'turkcell', 'ben', 'bok', 'yap', '?']\n",
      "Processed tweets saved to data/processed_tweets.pkl\n"
     ]
    }
   ],
   "source": [
    "# Path to the dataset\n",
    "dataset_path = \"data/raw_texts\"\n",
    "\n",
    "# Process the tweets\n",
    "processed_tweets = process_tweets(dataset_path,morphology, stop_words)\n",
    "\n",
    "# Print a sample\n",
    "for label, tweets in processed_tweets.items():\n",
    "    print(f\"\\nLabel: {label}\")\n",
    "    print(\"Sample Processed Tweet:\", tweets[0])\n",
    "\n",
    "# Save processed_tweets to a pickle file\n",
    "with open(\"data/processed_tweets.pkl\", \"wb\") as file:\n",
    "    pickle.dump(processed_tweets, file)\n",
    "\n",
    "print(\"Processed tweets saved to data/processed_tweets.pkl\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
