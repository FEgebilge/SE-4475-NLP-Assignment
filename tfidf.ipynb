{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Add the necessary imports for TF-IDF processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Processed Data for TF-IDF Input\n",
    "\n",
    "Prepare the data by flattening it into a list of documents and their associated labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tfidf_input(processed_tweets):\n",
    "    \"\"\"\n",
    "    Flattens the processed tweets dictionary into a list of documents and labels.\n",
    "    Args:\n",
    "        processed_tweets (dict): Dictionary with labels as keys and lists of tokenized tweets as values.\n",
    "    Returns:\n",
    "        tuple: (list of documents, list of labels)\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    labels = []\n",
    "    for label, tweets in processed_tweets.items():\n",
    "        for tweet in tweets:\n",
    "            documents.append(\" \".join(tweet))  # Join tokens into a single string for TF-IDF\n",
    "            labels.append(label)\n",
    "    return documents, labels\n",
    "\n",
    "# Example usage:\n",
    "documents, labels = prepare_tfidf_input(processed_tweets)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
