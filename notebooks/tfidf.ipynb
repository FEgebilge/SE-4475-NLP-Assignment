{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Add the necessary imports for TF-IDF processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Processed Data for TF-IDF Input\n",
    "Prepare the data by flattening it into a list of documents and their associated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tfidf_input(processed_tweets):\n",
    "    \"\"\"\n",
    "    Flattens the processed tweets dictionary into a list of documents and labels.\n",
    "    Args:\n",
    "        processed_tweets (dict): Dictionary with labels as keys and lists of tokenized tweets as values.\n",
    "    Returns:\n",
    "        tuple: (list of documents, list of labels)\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    labels = []\n",
    "    for label, tweets in processed_tweets.items():\n",
    "        for tweet in tweets:\n",
    "            documents.append(\" \".join(tweet))  # Join tokens into a single string for TF-IDF\n",
    "            labels.append(label)\n",
    "    logging.info(f\"Prepared {len(documents)} documents and {len(set(labels))} unique labels for TF-IDF vectorization.\")\n",
    "    return documents, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Compute TF-IDF\n",
    "Use TfidfVectorizer to transform the documents into a sparse TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(documents):\n",
    "    \"\"\"\n",
    "    Computes the TF-IDF matrix for the given documents.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of tokenized and preprocessed documents as strings.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (TF-IDF matrix, feature names)\n",
    "    \"\"\"\n",
    "    # Vectorizer with optimized settings\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=15000, \n",
    "        max_df=0.98,  \n",
    "        min_df=0.0002,  \n",
    "        ngram_range=(1,1)  # Use unigrams and bigrams\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Log the number of features after vectorization\n",
    "    logging.info(f\"Computed TF-IDF matrix with {tfidf_matrix.shape[1]} features (vocabulary size).\")\n",
    "    return tfidf_matrix, feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save TF-IDF as a CSV File\n",
    "Format the data and save it to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tfidf_to_csv(tfidf_matrix, feature_names, labels, output_path):\n",
    "    \"\"\"\n",
    "    Saves the TF-IDF matrix to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        tfidf_matrix (sparse matrix): Computed TF-IDF matrix.\n",
    "        feature_names (list): List of feature names (vocabulary).\n",
    "        labels (list): List of document labels.\n",
    "        output_path (str): Path to save the CSV file.\n",
    "    \"\"\"\n",
    "    # Convert sparse matrix to dense format and create DataFrame\n",
    "    tfidf_dense = tfidf_matrix.toarray()\n",
    "    tfidf_df = pd.DataFrame(tfidf_dense, columns=feature_names)\n",
    "    \n",
    "    # Add labels as a \"Class\" column\n",
    "    tfidf_df[\"Class\"] = labels\n",
    "\n",
    "    # Save to CSV\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)  # Ensure output directory exists\n",
    "    tfidf_df.to_csv(output_path, index=False)\n",
    "    logging.info(f\"TF-IDF matrix saved to '{output_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 22:20:46,243 - INFO - Processed tweets loaded successfully.\n",
      "2024-11-28 22:20:46,244 - INFO - Prepared 2999 documents and 3 unique labels for TF-IDF vectorization.\n",
      "2024-11-28 22:20:46,256 - INFO - Computed TF-IDF matrix with 2682 features (vocabulary size).\n",
      "2024-11-28 22:20:48,188 - INFO - TF-IDF matrix saved to '../reports/tfidf_values.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare input data\n",
    "# Load processed_tweets from the pickle file\n",
    "with open(\"../data/processed_tweets.pkl\", \"rb\") as file:\n",
    "    processed_tweets = pickle.load(file)\n",
    "\n",
    "logging.info(\"Processed tweets loaded successfully.\")\n",
    "documents, labels = prepare_tfidf_input(processed_tweets)\n",
    "\n",
    "# Step 2: Compute TF-IDF matrix\n",
    "tfidf_matrix, feature_names = compute_tfidf(documents)\n",
    "\n",
    "# Step 3: Save TF-IDF matrix to CSV\n",
    "output_csv_path = \"../reports/tfidf_values.csv\"\n",
    "save_tfidf_to_csv(tfidf_matrix, feature_names, labels, output_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
