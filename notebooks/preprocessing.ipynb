{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Turkish Tweets with Zemberek\n",
    "\n",
    "This notebook implements the preprocessing step of the project. It includes tokenization, lowercasing, stemming using Zemberek, and optional stop-word removal. The processed tweets will be saved for later steps, such as TF-IDF transformation and classification.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "import pickle\n",
    "import logging\n",
    "import re\n",
    "import emoji\n",
    "from jpype import JClass, JString, getDefaultJVMPath, startJVM\n",
    "from helpers import load_turkish_stop_words_from_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Zemberek for Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zemberek initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import Zemberek\n",
    "ZEMBEREK_PATH = \"../zemberek-full.jar\"\n",
    "\n",
    "# Configure logging to print the final summary\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Step 2: Initialize Zemberek\n",
    "\n",
    "def initialize_zemberek():\n",
    "    startJVM(getDefaultJVMPath(), '-ea', f'-Djava.class.path={ZEMBEREK_PATH}')\n",
    "    TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n",
    "    morphology = TurkishMorphology.createWithDefaults()\n",
    "    return morphology\n",
    "\n",
    "print(\"Zemberek initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Tokenization, Lowercasing, and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_preprocess_text(text, morphology, stop_words=[], custom_entities=[]):\n",
    "    \"\"\"\n",
    "    Advanced preprocessing for Turkish text using Zemberek: tokenizes, lemmatizes, removes stop words,\n",
    "    and handles mentions, hashtags, URLs, and emojis.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to preprocess.\n",
    "        morphology: Initialized TurkishMorphology instance.\n",
    "        stop_words (list): List of stop words to exclude.\n",
    "        custom_entities (list): List of custom named entities to be removed.\n",
    "\n",
    "    Returns:\n",
    "        list: List of preprocessed tokens.\n",
    "        dict: Dictionary containing the word/token count summary.\n",
    "    \"\"\"\n",
    "    initial_word_count = len(text.split())\n",
    "\n",
    "    # Step 1: Lowercase Conversion\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 2: Remove URLs, Mentions, Numbers, and Excess Whitespace\n",
    "    text = re.sub(r'http\\S+|www\\S+|@[A-Za-z0-9_]+|\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Step 3: Remove Hashtags (Keep text without '#')\n",
    "    text = re.sub(r'#', '', text)\n",
    "\n",
    "    # Step 4: Remove Custom Entities\n",
    "    for entity in custom_entities:\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(entity)), '', text)\n",
    "\n",
    "    # Step 5: Handle Emojis (Convert to Descriptive Text)\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    # Step 6: Tokenization (Basic Splitting)\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Step 7: Remove Short or Invalid Tokens\n",
    "    tokens = [t for t in tokens if len(t) > 2 and t.isalpha()]\n",
    "\n",
    "    # Step 8: Stop-word Removal\n",
    "    token_count_before_stop_word_removal = len(tokens)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    token_count_after_stop_word_removal = len(tokens)\n",
    "\n",
    "    # Step 9: Lemmatization with Zemberek\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        # Perform analysis\n",
    "        analysis = morphology.analyzeAndDisambiguate(JString(token)).bestAnalysis()\n",
    "\n",
    "        # Extract all possible lemmas\n",
    "        lemmas = [str(item.getLemmas()[0]) for item in analysis if item.getLemmas()]\n",
    "        if lemmas:\n",
    "            lemmatized_tokens.append(lemmas[0])  # Append the first lemma as a Python string\n",
    "        else:\n",
    "            lemmatized_tokens.append(token)  # Append the original token if no lemma is found\n",
    "\n",
    "    final_token_count = len(lemmatized_tokens)\n",
    "\n",
    "    # Step 10: Collect Summary Report\n",
    "    report = {\n",
    "        \"initial_word_count\": initial_word_count,\n",
    "        \"token_count_before_stop_word_removal\": token_count_before_stop_word_removal,\n",
    "        \"token_count_after_stop_word_removal\": token_count_after_stop_word_removal,\n",
    "        \"final_token_count\": final_token_count\n",
    "    }\n",
    "\n",
    "    return lemmatized_tokens, report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process All Tweets in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(dataset_folder, morphology, stop_words=[], custom_entities=[]):\n",
    "    \"\"\"\n",
    "    Processes all tweets in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_folder (str): Path to the dataset folder.\n",
    "        morphology: Initialized TurkishMorphology instance for stemming.\n",
    "        stop_words (list): List of stop words to exclude.\n",
    "        custom_entities (list): List of custom named entities to be removed.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with processed tweets by label.\n",
    "    \"\"\"\n",
    "    processed_data = {}\n",
    "    summary_reports = []\n",
    "\n",
    "    # Resolve absolute path for dataset folder\n",
    "    dataset_folder = os.path.abspath(dataset_folder)\n",
    "\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        raise FileNotFoundError(f\"Dataset folder not found: {dataset_folder}\")\n",
    "\n",
    "    # Iterate over label folders\n",
    "    for label in os.listdir(dataset_folder):\n",
    "        label_folder = os.path.join(dataset_folder, label)\n",
    "\n",
    "        # Skip non-directory entries\n",
    "        if not os.path.isdir(label_folder):\n",
    "            continue\n",
    "\n",
    "        tweets = []\n",
    "\n",
    "        # Iterate over files in the label folder\n",
    "        for filename in os.listdir(label_folder):\n",
    "            file_path = os.path.join(label_folder, filename)\n",
    "\n",
    "            # Skip non-text files\n",
    "            if not file_path.endswith(\".txt\"):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"ISO-8859-9\") as file:\n",
    "                    text = file.read().strip()\n",
    "                    if not text:\n",
    "                        continue\n",
    "\n",
    "                    # Preprocess text\n",
    "                    processed, report = advanced_preprocess_text(text, morphology, stop_words, custom_entities)\n",
    "\n",
    "                    # Convert all tokens to Python-native strings\n",
    "                    tweets.append([str(token) for token in processed])\n",
    "                    summary_reports.append(report)\n",
    "\n",
    "            except (FileNotFoundError, UnicodeDecodeError) as e:\n",
    "                logging.error(f\"Error in file: {file_path}, {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if tweets:\n",
    "            # Ensure label is a Python-native string\n",
    "            processed_data[str(label)] = tweets\n",
    "\n",
    "    # Generate summary report after processing all tweets\n",
    "    total_initial_words = sum([r[\"initial_word_count\"] for r in summary_reports])\n",
    "    total_tokens_before_stop_word_removal = sum([r[\"token_count_before_stop_word_removal\"] for r in summary_reports])\n",
    "    total_tokens_after_stop_word_removal = sum([r[\"token_count_after_stop_word_removal\"] for r in summary_reports])\n",
    "    total_final_tokens = sum([r[\"final_token_count\"] for r in summary_reports])\n",
    "\n",
    "    logging.info(f\"Summary Report for Preprocessing:\")\n",
    "    logging.info(f\"Total initial word count: {total_initial_words}\")\n",
    "    logging.info(f\"Total tokens before stop-word removal: {total_tokens_before_stop_word_removal}\")\n",
    "    logging.info(f\"Total tokens after stop-word removal: {total_tokens_after_stop_word_removal}\")\n",
    "    logging.info(f\"Total final token count after lemmatization: {total_final_tokens}\")\n",
    "\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I|21:38:05.162|Root lexicon created in 230 ms.                                                                     | DictionarySerializer#getDictionaryItems\n",
      "I|21:38:05.163|Dictionary generated in 301 ms                                                                      | RootLexicon#defaultBinaryLexicon\n",
      "I|21:38:05.334|Initialized in 506 ms.                                                                              | TurkishMorphology#createWithDefaults\n"
     ]
    }
   ],
   "source": [
    "# Initialize Zemberek\n",
    "morphology = initialize_zemberek()\n",
    "\n",
    "# Load stop words\n",
    "stop_words_csv_path = \"../data/stop_words.csv\"\n",
    "stop_words = load_turkish_stop_words_from_csv(stop_words_csv_path)\n",
    "\n",
    "# Custom entities to filter out\n",
    "custom_entities = ['nokia', 'panasonic', 'pepsi', 'istanbul', 'lig', 'kocaeli']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 21:38:07,421 - INFO - Summary Report for Preprocessing:\n",
      "2024-11-28 21:38:07,421 - INFO - Total initial word count: 35370\n",
      "2024-11-28 21:38:07,421 - INFO - Total tokens before stop-word removal: 25098\n",
      "2024-11-28 21:38:07,421 - INFO - Total tokens after stop-word removal: 19535\n",
      "2024-11-28 21:38:07,422 - INFO - Total final token count after lemmatization: 19535\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <java class 'java.lang.String'>: attribute lookup java.lang.String on jpype._jstring failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Save processed tweets to a pickle file\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/processed_tweets.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_tweets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed tweets saved to data/processed_tweets.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <java class 'java.lang.String'>: attribute lookup java.lang.String on jpype._jstring failed"
     ]
    }
   ],
   "source": [
    "# Path to the dataset\n",
    "dataset_path = \"../data/raw_texts\"\n",
    "\n",
    "# Process the tweets\n",
    "processed_tweets = process_tweets(dataset_path, morphology, stop_words, custom_entities)\n",
    "\n",
    "# Save processed tweets to a pickle file\n",
    "with open(\"../data/processed_tweets.pkl\", \"wb\") as file:\n",
    "    pickle.dump(processed_tweets, file)\n",
    "\n",
    "print(\"Processed tweets saved to data/processed_tweets.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
