{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Turkish Tweets with Zemberek\n",
    "\n",
    "This notebook implements the preprocessing step of the project. It includes tokenization, lowercasing, stemming using Zemberek, and optional stop-word removal. The processed tweets will be saved for later steps, such as TF-IDF transformation and classification.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "from zemberek import TurkishMorphology\n",
    "import pickle\n",
    "import logging\n",
    "import re\n",
    "import emoji\n",
    "from jpype import JClass, JString, getDefaultJVMPath, startJVM\n",
    "from helpers import load_turkish_stop_words_from_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Tokenization, Lowercasing, and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text,morphology):\n",
    "    # Remove punctuations\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    # Remove URLs, mentions, and numbers\n",
    "    text = re.sub(r'http\\S+|www\\S+|@[A-Za-z0-9_]+|\\d+', '', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Split into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Filter words (longer than 2 characters and alphabetic)\n",
    "    words = [w for w in words if len(w) > 2 and w.isalpha()]\n",
    "\n",
    "\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        results = morphology.analyze(word)\n",
    "        if results.analysis_results:\n",
    "            # If the word is not found, get the root form of the word.\n",
    "            lemma = results.analysis_results[0].get_stem()\n",
    "            processed_words.append(lemma)\n",
    "        else:\n",
    "            # If the word is found, add the word directly.\n",
    "            processed_words.append(word)\n",
    "\n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process All Tweets in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_folder, morphology):\n",
    "    \"\"\"\n",
    "    Process all texts in the dataset folder by preprocessing and labeling.\n",
    "\n",
    "    Args:\n",
    "        dataset_folder (str): Path to the dataset folder.\n",
    "        morphology: Zemberek morphology instance.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of preprocessed texts, list of labels)\n",
    "    \"\"\"\n",
    "    texts, labels = [], []\n",
    "    for label in os.listdir(dataset_folder):\n",
    "        label_folder = os.path.join(dataset_folder, label)\n",
    "        if not os.path.isdir(label_folder):\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(label_folder):\n",
    "            file_path = os.path.join(label_folder, filename)\n",
    "            if file_path.endswith(\".txt\"):\n",
    "                with open(file_path, \"r\", encoding=\"ISO-8859-9\") as file:\n",
    "                    text = file.read().strip()\n",
    "                    if text:\n",
    "                        processed_text = preprocess_text(text, morphology)\n",
    "                        texts.append(processed_text)\n",
    "                        labels.append(label)\n",
    "    \n",
    "    logging.info(f\"Processed {len(texts)} documents with {len(set(labels))} unique labels.\")\n",
    "    return texts, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-28 23:55:19,636 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 3.315598964691162\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "preprocess_text() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m dataset_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/raw_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Process tweets\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m text,labels \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmorphology\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Save texts and labels as a dictionary in a pickle file\u001b[39;00m\n\u001b[1;32m     18\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/processed_tweets.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[0;34m(dataset_folder, morphology)\u001b[0m\n\u001b[1;32m     22\u001b[0m text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text:\n\u001b[0;32m---> 24\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmorphology\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(processed_text)\n\u001b[1;32m     26\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "\u001b[0;31mTypeError\u001b[0m: preprocess_text() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Initialize Zemberek\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "\n",
    "# Load stop words\n",
    "stop_words_csv_path = \"../data/stop_words.csv\"\n",
    "stop_words = load_turkish_stop_words_from_csv(stop_words_csv_path)\n",
    "\n",
    "# Define custom entities\n",
    "custom_entities = [\"nokia\", \"panasonic\", \"pepsi\", \"istanbul\", \"lig\", \"kocaeli\"]\n",
    "\n",
    "# Path to the dataset\n",
    "dataset_folder = \"../data/raw_texts\"\n",
    "\n",
    "# Process tweets\n",
    "text,labels = process_dataset(dataset_folder, morphology)\n",
    "\n",
    "# Save texts and labels as a dictionary in a pickle file\n",
    "output_path = \"../data/processed_tweets.pkl\"\n",
    "with open(output_path, \"wb\") as file:\n",
    "    pickle.dump({\"texts\": text, \"labels\": labels}, file)\n",
    "\n",
    "logging.info(f\"Processed tweets and labels saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
